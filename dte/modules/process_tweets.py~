

from luiginlp.engine import Task, StandardWorkflowComponent, InputFormat, InputComponent, registercomponent, InputSlot, Parameter, BoolParameter

import json

from modules import filter_tweets, tokenize_instances, extract_daterefs, extract_cityref, extract_entities
        
@registercomponent
class ProcessTweets(WorkflowComponent):

    tweets = Parameter()

    skip_date = BoolParameter()
    skip_month = BoolParameter()
    skip_timeunit = BoolParameter()
    skip_day = BoolParameter()
    citylist = Parameter()
    commonness_txt = Parameter()
    commonness_cls = Parameter()
    commonness_corpus = Parameter()
    ngrams_score = Parameter()    
    
    def accepts(self):
        return InputFormat(self, format_id='tweets', extension='.gz')
                    
    def setup(self, workflow, input_feeds):
        
        filterer = workflow.new_task('filter_tweets', filter_tweets.FilterTweetsTask, autopass=True)
        filterer.in_tweets = input_feeds['tweets']

        tokenizer = workflow.new_task('tokenize_tweets', tokenize_instances.Tokenize_instances, autopass=True)
        tokenizer.in_filtered = filterer.out_filtered

        dateref_extractor = workflow.new_task('extract_daterefs', extract_daterefs.ExtractDaterefTask, autopass=True, skip_date=self.skip_date, skip_month=self.skip_month, skip_timeunit=self.skip_timeunit, skip_day=self.skip_day)  
        dateref_extractor.in_tokenized = filterer.out_tokenized

        cityref_extractor = workflow.new_task('extract_cityref', extract_cityref.ExtractCityrefTask, autopass=True, citylist=self.citylist)
        cityref_extractor.in_dateref = dateref_extractor.out_dateref

        entity_extractor = workflow.new_task('extract_entities', extract_entities.ExtractEntitiesTask, autopass=True, commonness_txt=self.commonness_txt, commonness_cls=self.commonness_cls, commonness_corpus=self.commonness_corpus, ngrams_score=self.ngrams_score)
        entity_extractor.in_cityref = cityref_extractor.out_cityref

        return entity_extractor
